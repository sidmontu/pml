---
title: "Practical Machine Learning Project"
output: html_document
---

### Executive Summary

In this report, we detail how we built a machine-learnt model on a dataset that contains readings from accelerometers on the belt, forearm, arm, and dumbell that were used by 6 participants. The participants were tasked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal is to predict the manner in which they did the exercise ("classe" column), based on any of the feature variables in the dataset. We build a model on the supplied training set, achieving an accuracy of 99.24% on a test set. We also use our model to give predictions for a new test set that does not have an outcome column supplied (i.e. unlabeled dataset).


### Data sources

From the course project guidelines page:

> The training data for this project are available here:
>
> https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
>
> The test data are available here:
>
> https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
>
> The data for this project comes from this original source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

I download these CSV files, and rename them to "train.csv", and "test.csv" respectively, for ease of readability.

### Setup

We start off first by loading some packages:
```{r,results="hide",message=FALSE}
library(ggplot2);library(lattice);library(reshape2);library(ggthemes);library(grid);library(caret);library(randomForest);
```

For reproduceability, you can set the seed as:
```{r}
set.seed(123)
```

Let's load in the data:
```{r}
training <- read.csv("train.csv",header=T,sep=",")
testing <- read.csv("test.csv",header=T,sep=",")
```

Next, we clean the data. There are a lot of NA columns in the dataset. We remove NA columns, and non-numeric columns as follows:
```{r}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
training_classe <- training$classe
training <- training[, sapply(training, is.numeric)]
testing <- testing[, sapply(testing, is.numeric)]
```

We can also remove columns that contain useless prediction variables such as "num_window" and timestamps. Observing the dataset, we notice that the first 4 columns contain these metadata, and hence, we remove the first seven columns as follows:
```{r}
training <- training[,-c(seq(1:4))]
testing <- testing[,-c(seq(1:4))]
training$classe <- training_classe
```
The final datasets should look as follows:
```{r}
dim(training)
dim(testing)
```
Note that they both have 53 columns: train has a "classe" outcome column, while test has a "problem_id" identifier column. Hence, there are a total of 52 features we will use to train our model.

Finally, let's split our training data into training and test datasets, such that we create the best unbiased model for eventual testing on the provided test set. We split into 70-30 ratios respectively:
```{r}
inTrain <- createDataPartition(training$classe, p=0.7, list=F)
trainSet <- training[inTrain,]
testSet <- training[-inTrain,]
```

### Modeling the data

To build a machine learning model, we use the Random Forest algorithm, which is observed to produce accurate models.

```{r,eval=FALSE}
modFit <- train(classe~.,data=trainSet,method="rf")
```

Random forest, while it gives great accuracy, is however a slow algorithm, as it expands into an expensive tree-based search. I ran this search offline, and in the interest of time, I am simply documenting results in this Rmarkdown report, instead of re-running the train function over and over again whenever knitr is called to recompile this document. The reader can run these commands to verify the functionality by changing all the R-code chunks from "eval=FALSE" to "eval=TRUE", and commenting out all the verbatim output chunks pasted in this report.

The output model from the Random Forest runs produces the following output:
```{r,eval=FALSE}
modFit

> Random Forest 
>
> 13737 samples
>
>   52 predictors
>
>    5 classes: 'A', 'B', 'C', 'D', 'E' 
>
> No pre-processing
>
> Resampling: Bootstrapped (25 reps) 
>
> Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
>
> Resampling results across tuning parameters:
>
>  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD
>
>   2    0.9882319  0.9851134  0.001688189  0.002137479
>
>  27    0.9890607  0.9861609  0.001991568  0.002528373
>
>  52    0.9818850  0.9770850  0.005821735  0.007368116
>
> Accuracy was used to select the optimal model using  the largest value.
>
> The final value used for the model was mtry = 27. 
```

We can see the accuracy on the training set is pretty high at approximately 98.9%. We can now test our model on the testing set:

```{r,eval=FALSE}
predictions <- predict(modFit, testSet)
confusionMatrix(testSet$classe, predictions)

> Confusion Matrix and Statistics
>
>          Reference
>
> Prediction    A    B    C    D    E
>
>         A 1673    1    0    0    0
>
>         B    8 1128    3    0    0
>
>         C    0    9 1014    3    0
>
>         D    0    0   17  946    1
>
>         E    0    0    2    1 1079
>
> Overall Statistics
>                                          
>               Accuracy : 0.9924          
>
>               95% CI : (0.9898, 0.9944)
>
>               No Information Rate : 0.2856          
>
>    P-Value [Acc > NIR] : < 2.2e-16       
>                                          
>                  Kappa : 0.9903          
>
> Mcnemar's Test P-Value : NA              
>
> Statistics by Class:
>
>                     Class: A Class: B Class: C Class: D Class: E
>
> Sensitivity            0.9952   0.9912   0.9788   0.9958   0.9991
>
> Specificity            0.9998   0.9977   0.9975   0.9964   0.9994
>
> Pos Pred Value         0.9994   0.9903   0.9883   0.9813   0.9972
>
> Neg Pred Value         0.9981   0.9979   0.9955   0.9992   0.9998
>
> Prevalence             0.2856   0.1934   0.1760   0.1614   0.1835
>
> Detection Rate         0.2843   0.1917   0.1723   0.1607   0.1833
>
> Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
>
> Balanced Accuracy      0.9975   0.9944   0.9881   0.9961   0.9992
```

We obtain a high **99.24% accuracy**. That gives us an estimated **out of sample error of 0.76%**.

### Predictions on the test set

We now use our built model to predict on the provided test dataset.

```{r,eval=FALSE}
predict(modFit, testing[,-c(53)])

> [1] B A B A A E D B A A B C B A E E A B B B
> Levels: A B C D E
```